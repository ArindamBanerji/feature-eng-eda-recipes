{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest importance\n",
    "\n",
    "Random forests is one the most popular machine learning algorithms. It is so successful because it provide good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Random forests consist typically of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or a combination of features. At each node (that is, at each question), the three divides the dataset in 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is. \n",
    "\n",
    "For classification, the measure of impurity is either Gini or the entropy. For regression the  measure of impurity is the variance. When training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease elicited by each feature is averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "In general, features that are selected at the top of the trees are more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "**Note**\n",
    "- Random Forests and decision trees in general give preference to features with high cardinality\n",
    "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
    "\n",
    "I will demonstrate how to select features based on tree importance using a regression and classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IoT_thoughts\\\\python-projects\\\\kaggle_experiments\\\\feature-engineering\\\\recipes\\\\notebooks\\\\feat_eng_select', 'C:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IOT_thoughts\\\\python-projects\\\\kaggle_experiments\\\\feature-engineering\\\\recipes\\\\helper_src', 'c:\\\\Python310\\\\python310.zip', 'c:\\\\Python310\\\\DLLs', 'c:\\\\Python310\\\\lib', 'c:\\\\Python310', '', 'c:\\\\Python310\\\\lib\\\\site-packages', 'c:\\\\Python310\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Python310\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Python310\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from helper_fe_v2 import (\n",
    "            get_full_datapath_nm,\n",
    "            read_df_from_file,\n",
    "            check_module_members,\n",
    "            gen_correlation,\n",
    "            do_bkwd_fwd_selection,\n",
    "            yaml_path,\n",
    "            read_yaml_conf,\n",
    "            remove_duplicates, \n",
    "            drop_const_features,\n",
    "            drop_quasi_const_features ,\n",
    "            run_randomForestClassifier,\n",
    "            run_logistic,\n",
    "            run_randomForestRegressor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml_ conf  {'write_file': True, 'base_dir': 'C:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IOT_thoughts\\\\python-projects\\\\kaggle_experiments', 'full_config_file': 'C:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IOT_thoughts\\\\python-projects\\\\kaggle_experiments\\\\feature-engineering\\\\recipes\\\\py-projects.yaml', 'current_proj_dir': 'C:\\\\Users\\\\Arindam Banerji\\\\CopyFolder\\\\IOT_thoughts\\\\python-projects\\\\kaggle_experiments\\\\feature-engineering\\\\recipes', 'data_dir_nm': 'input_data\\raw', 'files': {'test_data_set2': 'fselect_dataset_2.csv', 'housing_data': 'housing_prices_train.csv', 'test_data_set1': 'fselect_dataset_1.csv', 'titanic_data': 'fe-cookbook-titanic.csv', 'comp_eda_file': 'none.csv'}, 'project_parms': {'use_mlxtnd': 'False'}, 'process_eda': {'main_file': 'fselect_dataset_2.csv', 'compre_file': 'none.csv', 'pairwise_analysis': 'on', 'show_html': 'False'}, 'RandomForestConfig': {'n_estimators': 200, 'rand_state': 39, 'max_depth': 4}}\n"
     ]
    }
   ],
   "source": [
    "config = read_yaml_conf(yaml_path())\n",
    "print (\"yaml_ conf \", config ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path NM exists  C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_2.csv\n",
      "full_path nm -from read_df C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_df_from_file ( config['files']['test_data_set2'], set_nrows=False, nrws=0 ) \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we fit Random Forests and select features in 2 lines of code\n",
    "\n",
    "# first I specify the Random Forest instance and its parameters\n",
    "\n",
    "# Then I use the selectFromModel class from sklearn\n",
    "# to automatically select the features\n",
    "\n",
    "# SelectFrom model will select those features which importance\n",
    "# is greater than the mean importance of all the features\n",
    "# by default, but you can alter this threshold if you want to\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
    "\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False, False,  True, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True, False,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True, False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were selected.\n",
    "\n",
    "# sklearn will select those features which importance values\n",
    "# are greater than the mean of all the coefficients.\n",
    "\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a list and count the selected features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16',\n",
       "       'var_17', 'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55',\n",
       "       'var_60', 'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87',\n",
       "       'var_88', 'var_95', 'var_96', 'var_99', 'var_103', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the distribution of importances\n",
    "\n",
    "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist(bins=20)\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 27\n",
      "features with importance greater than the mean importance of all features: 27\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which importance is above the\n",
    "# mean of all features, to make sure we understand the output of\n",
    "# SelectFromModel\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with importance greater than the mean importance of all features: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- If we change the parameters of the tree, we may obtain different features\n",
    "- How many features to select is somewhat arbitrary\n",
    "\n",
    "With SelectFromModel we use the mean of all importances as threshold. We can modify this threshold within the SelectFromModel if we want more or less features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path NM exists  C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\housing_prices_train.csv\n",
      "full_path nm -from read_df C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\housing_prices_train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_df_from_file ( config['files']['housing_data'], set_nrows=False, nrws=0 ) \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestRegressor(random_state=10))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train a random forest for regression and select features\n",
    "# in 2 lines of code\n",
    "\n",
    "# SelectFrom model will select those features which importance\n",
    "# is greater than the mean importance of all the features\n",
    "# by default, but you can alter this threshold if you want to\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make a list and count the selected features\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 37\n",
      "selected features: 5\n",
      "features with coefficients greater than the mean coefficient: 5\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  amount of selected features\n",
    "# with the amount of features which importance is above the\n",
    "# mean importance, to make sure we understand the output of\n",
    "# sklearn\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a very straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, as I said, correlated features will show in a tree similar importance, but lower than compared to what their importance would be if the tree was built without the correlated counterparts.\n",
    "\n",
    "In situations like this, it is better to select features recursively, rather than altogether like we are doing in this lecture.\n",
    "\n",
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Selection using Random Forests importance\n",
    "\n",
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance by brute force like we did in the previous notebook, we could get a better selection by removing one feature at a time, and recalculating the importance on each round. This procedure is called Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE is a hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build Random Forests using all features\n",
    "- Remove least important feature\n",
    "- Build Random Forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better feature space selection. On the downside, building several Random Forests is quite time and compute resource consuming, in particular if the dataset contains a high number of features.\n",
    "\n",
    "I will demonstrate how to select features based Random Forests importance recursively using sklearn on a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path NM exists  C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_2.csv\n",
      "full_path nm -from read_df C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml_conf(yaml_path())\n",
    "data = read_df_from_file ( config['files']['test_data_set2'], set_nrows=False, nrws=0 ) \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=10),\n",
       "    n_features_to_select=27)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do model training and feature selection in 2 lines of code\n",
    "\n",
    "# first I specify the Random Forest and its parameters\n",
    "\n",
    "# Then RFE from sklearn to remove features recursively\n",
    "\n",
    "# RFE will remove one feature at each iteration => the least  important.\n",
    "# then it will build another random forest and repeat\n",
    "# till a criteria is met.\n",
    "\n",
    "# in sklearn the criteria to stop is an arbitrary number\n",
    "# of features to select, that we need to decide before hand\n",
    "# not the best solution, but a solution\n",
    "\n",
    "sel_ = RFE(RandomForestClassifier(n_estimators=10, random_state=10), n_features_to_select=27)\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_4', 'var_14', 'var_21', 'var_25', 'var_30', 'var_31', 'var_34',\n",
       "       'var_38', 'var_41', 'var_46', 'var_53', 'var_55', 'var_56', 'var_70',\n",
       "       'var_71', 'var_73', 'var_79', 'var_82', 'var_86', 'var_87', 'var_88',\n",
       "       'var_90', 'var_92', 'var_93', 'var_96', 'var_104', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's display the list of features\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the features selected in the previous\n",
    "# notebook where we used SelectFromModel from sklearn\n",
    "# without doing it recursively\n",
    "\n",
    "previous_lecture_selected_features = [\n",
    "    'var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16', 'var_17',\n",
    "    'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55', 'var_60',\n",
    "    'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87', 'var_88',\n",
    "    'var_95', 'var_96', 'var_99', 'var_103', 'var_108'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7046591498448564\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6904472637540937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.37253398, 0.62746602],\n",
       "       [0.21224039, 0.78775961],\n",
       "       [0.19444672, 0.80555328],\n",
       "       ...,\n",
       "       [0.25807847, 0.74192153],\n",
       "       [0.38462335, 0.61537665],\n",
       "       [0.38994431, 0.61005569]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features selected recursively\n",
    "run_randomForestClassifier(X_train[selected_feat],\n",
    "                  X_test[selected_feat],\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7126509093226299\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7009195513033531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.38095524, 0.61904476],\n",
       "       [0.21765451, 0.78234549],\n",
       "       [0.2208501 , 0.7791499 ],\n",
       "       ...,\n",
       "       [0.19208147, 0.80791853],\n",
       "       [0.39994195, 0.60005805],\n",
       "       [0.39428315, 0.60571685]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features selected altogether\n",
    "run_randomForestClassifier(X_train[previous_lecture_selected_features],\n",
    "                  X_test[previous_lecture_selected_features],\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that RFE did not return a better subset of features. The performance of the model built using the variables selected directly from the first RandomForest was enough to find a good subset of features.\n",
    "\n",
    "In my opinion the RFE from sklearn does not bring forward a massive advantage respect to the SelectFromModel method and personally I tend to use the second to select my features.\n",
    "\n",
    "That is all for this lecture, I hope you enjoyed it and see you in the next one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection with decision trees, review\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path NM exists  C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_1.csv\n",
      "full_path nm -from read_df C:\\Users\\Arindam Banerji\\CopyFolder\\IOT_thoughts\\python-projects\\kaggle_experiments\\feature-engineering\\recipes\\input_data\\raw\\fselect_dataset_1.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = read_yaml_conf(yaml_path())\n",
    "data = read_df_from_file ( config['files']['test_data_set1'], set_nrows=False, nrws=0 ) \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset with all the variables\n",
    "# to compare the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 266) (15000, 266)\n"
     ]
    }
   ],
   "source": [
    "constant_features = drop_const_features(X_train, X_test, drop_feat='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 215) (15000, 215)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, features_to_keep = drop_quasi_const_features(threshold=0.01,\n",
    "                                                              X_train=X_train, \n",
    "                                                              X_test=X_test, \n",
    "                                                              drop_feat='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "(35000, 205) (15000, 205)\n"
     ]
    }
   ],
   "source": [
    "duplicated_feat = remove_duplicates(X_train, X_test, drop_dup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset without constant, quasi-constant and duplicated variables\n",
    "# to measure the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  93\n"
     ]
    }
   ],
   "source": [
    "# remove correlated features to reduce the feature space\n",
    "# remove correlated features to reduce the feature space\n",
    "\n",
    "corr_features = gen_correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataset at  this stage\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the impotance derived from\n",
    "# random forests\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=10))\n",
    "sel_.fit(X_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient from dataset\n",
    "# and parse again as dataframe (output of sklearn is\n",
    "# numpy array)\n",
    "X_train_rf = pd.DataFrame(sel_.transform(X_train))\n",
    "X_test_rf = pd.DataFrame(sel_.transform(X_test))\n",
    "\n",
    "# add the columns name\n",
    "X_train_rf.columns = X_train.columns[(sel_.get_support())]\n",
    "X_test_rf.columns = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 16), (15000, 16))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.807612232524249\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7868832427636059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9830392 , 0.0169608 ],\n",
       "       [0.92255307, 0.07744693],\n",
       "       [0.97249843, 0.02750157],\n",
       "       ...,\n",
       "       [0.98297334, 0.01702666],\n",
       "       [0.98246554, 0.01753446],\n",
       "       [0.9199013 , 0.0800987 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "run_randomForestClassifier(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.810290026780428\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7914020645941601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98450622, 0.01549378],\n",
       "       [0.91940171, 0.08059829],\n",
       "       [0.97123244, 0.02876756],\n",
       "       ...,\n",
       "       [0.98453254, 0.01546746],\n",
       "       [0.98462966, 0.01537034],\n",
       "       [0.91597684, 0.08402316]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "run_randomForestClassifier(X_train_basic_filter,\n",
    "                  X_test_basic_filter,\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98443145, 0.01556855],\n",
       "       [0.92607865, 0.07392135],\n",
       "       [0.96483524, 0.03516476],\n",
       "       ...,\n",
       "       [0.98441518, 0.01558482],\n",
       "       [0.98346227, 0.01653773],\n",
       "       [0.92007532, 0.07992468]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "run_randomForestClassifier(X_train_corr,\n",
    "                  X_test_corr,\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.98443145, 0.01556855],\n",
       "       [0.92607865, 0.07392135],\n",
       "       [0.96483524, 0.03516476],\n",
       "       ...,\n",
       "       [0.98441518, 0.01558482],\n",
       "       [0.98346227, 0.01653773],\n",
       "       [0.92007532, 0.07992468]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForestClassifier(X_train_corr,\n",
    "                  X_test_corr,\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.825594244784318\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8037861254524954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99057627, 0.00942373],\n",
       "       [0.94289959, 0.05710041],\n",
       "       [0.95476487, 0.04523513],\n",
       "       ...,\n",
       "       [0.98991585, 0.01008415],\n",
       "       [0.98888135, 0.01111865],\n",
       "       [0.93272561, 0.06727439]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedded methods - Random forests\n",
    "run_randomForestClassifier(X_train_rf,\n",
    "                  X_test_rf,\n",
    "                  y_train, y_test, yaml_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Random Forests built using 16 features display a slightly higher performance than a model built with all features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
